{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-lawsuit",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educational-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# 네이버 영화 리뷰 불러오기\n",
    "data_path = os.path.dirname(os.path.abspath('__file__')) + r'/'\n",
    "train_data = pd.read_table(data_path + 'ratings_train.txt')\n",
    "test_data = pd.read_table(data_path + 'ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "characteristic-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 : 150000\n",
      "리뷰 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 :',len(train_data)) # 리뷰 개수 출력\n",
    "print('리뷰 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boolean-comfort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-pearl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49157, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['document'].nunique(), test_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "consecutive-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controversial-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 : 146183\n",
      "리뷰 개수 : 49158\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 :',len(train_data)) # 리뷰 개수 출력\n",
    "print('리뷰 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "associate-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  73342\n",
      "1      1  72841\n"
     ]
    }
   ],
   "source": [
    "print(train_data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advised-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerical-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 .!?]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거 + 온점 + ! + ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "helpful-wrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dependent-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "document    406\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "matched-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145777\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surrounded-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 .!?]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거 + 온점 + ! + ?\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rough-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "marine-routine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intensive-alliance",
   "metadata": {},
   "source": [
    "토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adolescent-recorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train_data, test_data])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-standing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cordless-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(df['document']))\n",
    "    #샘플을 naver_review.txt 파일에 저장한 후에 센텐스피스를 통해 단어 집합을 생성\n",
    "with open('naver_review_train.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_data['document']))\n",
    "with open('naver_review_test.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(test_data['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "about-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=18000 --model_type=bpe --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver.model, naver.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turned-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spm.SentencePieceTrainer.Train('--input=naver_review_train.txt --model_prefix=naver_review_train --vocab_size=18000 --model_type=bpe --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver_review_train.model, naver_review_train.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "colored-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spm.SentencePieceTrainer.Train('--input=naver_review_test.txt --model_prefix=naver_review_test --vocab_size=18000 --model_type=bpe --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver_review_test.model, naver_review_test.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "royal-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "됨\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "#train_vocab_list = pd.read_csv('naver_review_train.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "#test_vocab_list = pd.read_csv('naver_review_test.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "print(\"됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "athletic-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "south-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adjacent-mining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "flush-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뭐 이딴 것도 영화냐.\n",
      "['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\n",
      "[130, 945, 1250, 2634, 16461]\n",
      "\n",
      "진짜 최고의 영화입니다 ㅋㅋ\n",
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "[54, 207, 806, 86]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "  \"뭐 이딴 것도 영화냐.\",\n",
    "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
    "]\n",
    "for line in lines:\n",
    "  print(line)\n",
    "  print(sp.encode_as_pieces(line))\n",
    "  print(sp.encode_as_ids(line))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "amino-reminder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "frozen-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영화'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.IdToPiece(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "difficult-priest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bibliographic-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'진짜 최고의 영화입니다 ᄏᄏ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds([54, 207, 806, 86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sudden-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "x_train = np.array(train_data['document'])\n",
    "x_test = np.array(test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "polish-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "suffering-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./naver.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "correct-helena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    7  1030     3 ...     0     0     0]\n",
      " [ 1611     8  4907 ...     0     0     0]\n",
      " [   23 16495     0 ...     0     0     0]\n",
      " ...\n",
      " [  244   130 16490 ...     0     0     0]\n",
      " [ 2007   563  5265 ...     0     0     0]\n",
      " [  227     5 10842 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = x_train\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_train = tensor\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "funny-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1215   445     0 ...     0     0     0]\n",
      " [ 1375     6    70 ...     0     0     0]\n",
      " [ 2611 16464  5574 ...     0     0     0]\n",
      " ...\n",
      " [16173   821  1915 ...     0     0     0]\n",
      " [  778  1408 16464 ...     0     0     0]\n",
      " [ 5053   319   254 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = x_test\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_test = tensor\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "desirable-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fresh-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(18000, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "antique-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "earlier-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1944/1944 [==============================] - 64s 31ms/step - loss: 0.6935 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49582, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "1944/1944 [==============================] - 54s 28ms/step - loss: 0.6932 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49582 to 0.50418, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "1944/1944 [==============================] - 55s 28ms/step - loss: 0.6932 - acc: 0.4961 - val_loss: 0.6935 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50418\n",
      "Epoch 4/15\n",
      "1944/1944 [==============================] - 56s 29ms/step - loss: 0.6933 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50418\n",
      "Epoch 5/15\n",
      "1944/1944 [==============================] - 56s 29ms/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50418\n",
      "Epoch 6/15\n",
      "1944/1944 [==============================] - 57s 29ms/step - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50418\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "theoretical-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531/1531 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4977\n",
      "\n",
      " 테스트 정확도: 0.4977\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-novelty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "antique-plenty",
   "metadata": {},
   "source": [
    "망해서 다시해봄\n",
    "sp의 단어사이즈 1.8만에서 3만으로 늘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "finished-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=30000 --model_type=bpe --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver.model, naver.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "secondary-contest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "auburn-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "x_train = np.array(train_data['document'])\n",
    "x_test = np.array(test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "chicken-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    7  1030     3 ...     0     0     0]\n",
      " [ 1611     8  4907 ...     0     0     0]\n",
      " [   23 28495     0 ...     0     0     0]\n",
      " ...\n",
      " [  244   130 28490 ...     0     0     0]\n",
      " [ 2007   563  5265 ...     0     0     0]\n",
      " [  227     5 10842 ...     0     0     0]]\n",
      "[[ 1215   445     0 ...     0     0     0]\n",
      " [ 1375     6    70 ...     0     0     0]\n",
      " [ 2611 28464  5574 ...     0     0     0]\n",
      " ...\n",
      " [16173   821 24406 ...     0     0     0]\n",
      " [  778 18056    38 ...     0     0     0]\n",
      " [21816   254  3766 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./naver.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word\n",
    "\n",
    "\n",
    "\n",
    "my_corpus = x_train\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_train = tensor\n",
    "print(x_train)\n",
    "\n",
    "my_corpus = x_test\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_test = tensor\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "touched-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1944/1944 [==============================] - 66s 33ms/step - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50418, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "1944/1944 [==============================] - 62s 32ms/step - loss: 0.6933 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50418\n",
      "Epoch 3/15\n",
      "1944/1944 [==============================] - 61s 31ms/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50418\n",
      "Epoch 4/15\n",
      "1944/1944 [==============================] - 58s 30ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50418\n",
      "Epoch 5/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50418\n",
      "Epoch 6/15\n",
      "1944/1944 [==============================] - 61s 31ms/step - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50418\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(30000, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "express-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531/1531 [==============================] - 12s 8ms/step - loss: 0.6932 - acc: 0.4977\n",
      "\n",
      " 테스트 정확도: 0.4977\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-robertson",
   "metadata": {},
   "source": [
    "학습 다 안 됐지만 이미 망한거 같아서 다시 해봄\n",
    "단어장 사이즈 5000으로, 모델을 unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "commercial-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver3 --vocab_size=5000 --model_type=unigram --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver3.model, naver3.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "iraqi-joseph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('naver3.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver3.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "nearby-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "x_train = np.array(train_data['document'])\n",
    "x_test = np.array(test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "finite-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  52 1018    6 ...    0    0    0]\n",
      " [2041   10  603 ...    0    0    0]\n",
      " [  25  374    0 ...    0    0    0]\n",
      " ...\n",
      " [ 176  152   47 ...    0    0    0]\n",
      " [2204  760  114 ...    0    0    0]\n",
      " [ 270    7 4076 ...    0    0    0]]\n",
      "[[2678  369    0 ...    0    0    0]\n",
      " [1438   27   94 ...    0    0    0]\n",
      " [2285   12  576 ...    0    0    0]\n",
      " ...\n",
      " [2502   11  796 ...    0    0    0]\n",
      " [ 699 1538   12 ...    0    0    0]\n",
      " [2124   12  236 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = x_train\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_train = tensor\n",
    "print(x_train)\n",
    "\n",
    "my_corpus = x_test\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_test = tensor\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "meaningful-internship",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1944/1944 [==============================] - 61s 30ms/step - loss: 0.6936 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49582, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "1944/1944 [==============================] - 58s 30ms/step - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49582 to 0.50418, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "1944/1944 [==============================] - 58s 30ms/step - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50418\n",
      "Epoch 4/15\n",
      "1944/1944 [==============================] - 58s 30ms/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50418\n",
      "Epoch 5/15\n",
      "  83/1944 [>.............................] - ETA: 51s - loss: 0.6932 - acc: 0.4966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-5364a2f34cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-baghdad",
   "metadata": {},
   "source": [
    "또 학습 멈추길래 그냥 중지시킴\n",
    "이번엔 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "worst-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver4 --vocab_size=8000 --model_type=char --max_sentence_length=9999')\n",
    "#vocab 생성이 완료되면 naver3.model, naver3.vocab 파일 두개가 생성 됩니다. .vocab 에서 학습된 subwords를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "precious-candle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('naver4.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver4.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "certified-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "x_train = np.array(train_data['document'])\n",
    "x_test = np.array(test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "judicial-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3  15   3 ...   0   0   0]\n",
      " [  3 669   4 ...   0   0   0]\n",
      " [  3  72  41 ...   0   0   0]\n",
      " ...\n",
      " [  3   5  27 ...   0   0   0]\n",
      " [  3 248 576 ...   0   0   0]\n",
      " [  3  19 135 ...   0   0   0]]\n",
      "[[  3 579   3 ...   0   0   0]\n",
      " [  3 196  82 ...   0   0   0]\n",
      " [  3   9 139 ...   0   0   0]\n",
      " ...\n",
      " [  3  37 363 ...   0   0   0]\n",
      " [  3 238  47 ...   0   0   0]\n",
      " [  3  49  41 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = x_train\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_train = tensor\n",
    "print(x_train)\n",
    "\n",
    "my_corpus = x_test\n",
    "tensor, word_index, index_word = sp_tokenize(sp, my_corpus)\n",
    "x_test = tensor\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "athletic-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1944/1944 [==============================] - 62s 31ms/step - loss: 0.6937 - acc: 0.5028 - val_loss: 0.6616 - val_acc: 0.5585\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55851, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.6606 - acc: 0.6038 - val_loss: 0.5961 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55851 to 0.71666, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.5014 - acc: 0.7558 - val_loss: 0.4367 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71666 to 0.79160, saving model to best_model.h5\n",
      "Epoch 4/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.4140 - acc: 0.8089 - val_loss: 0.4193 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79160 to 0.80735, saving model to best_model.h5\n",
      "Epoch 5/15\n",
      "1944/1944 [==============================] - 59s 31ms/step - loss: 0.3823 - acc: 0.8277 - val_loss: 0.3918 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.80735 to 0.81626, saving model to best_model.h5\n",
      "Epoch 6/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.3641 - acc: 0.8359 - val_loss: 0.3654 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81626 to 0.83286, saving model to best_model.h5\n",
      "Epoch 7/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.3473 - acc: 0.8458 - val_loss: 0.3808 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83286\n",
      "Epoch 8/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.3332 - acc: 0.8528 - val_loss: 0.3481 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83286 to 0.84549, saving model to best_model.h5\n",
      "Epoch 9/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.3146 - acc: 0.8632 - val_loss: 0.3443 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.84549 to 0.84823, saving model to best_model.h5\n",
      "Epoch 10/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.3061 - acc: 0.8667 - val_loss: 0.3471 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84823\n",
      "Epoch 11/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.2946 - acc: 0.8742 - val_loss: 0.3467 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84823 to 0.85399, saving model to best_model.h5\n",
      "Epoch 12/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.2791 - acc: 0.8815 - val_loss: 0.3537 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85399\n",
      "Epoch 13/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.2658 - acc: 0.8882 - val_loss: 0.3430 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85399\n",
      "Epoch 14/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.2527 - acc: 0.8958 - val_loss: 0.3584 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85399\n",
      "Epoch 15/15\n",
      "1944/1944 [==============================] - 60s 31ms/step - loss: 0.2361 - acc: 0.9043 - val_loss: 0.3492 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85399\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "russian-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.3562 - acc: 0.8484\n",
      "\n",
      " 테스트 정확도: 0.8484\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-velvet",
   "metadata": {},
   "source": [
    "vocab_size=8000 --model_type=char 로 했더니 결과가 꽤 괜찮은 것 같다!  \n",
    "이제 센텐스피스 없이 학습했을 때랑 비교..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "unauthorized-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "missing-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.dirname(os.path.abspath('__file__')) + r'/'\n",
    "train_data = pd.read_table(data_path + 'ratings_train.txt')\n",
    "test_data = pd.read_table(data_path + 'ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "compatible-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "document    406\n",
      "label         0\n",
      "dtype: int64\n",
      "id            0\n",
      "document    169\n",
      "label         0\n",
      "dtype: int64\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 .!?]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거 + 온점 + ! + ?\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 .!?]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거 + 온점 + ! + ?\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "nasty-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "duplicate-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-republican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
